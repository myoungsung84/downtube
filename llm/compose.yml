name: downtube-llm

services:
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: downtube-llm
    ports:
      - "${LLM_PORT:-18080}:${LLM_PORT:-18080}"
    volumes:
      - ./models:/models:ro
    command:
      - -m
      - /models/${MODEL_FILE}
      - -c
      - "8192"
      - --host
      - 0.0.0.0
      - --port
      - "${LLM_PORT:-18080}"
    restart: unless-stopped
